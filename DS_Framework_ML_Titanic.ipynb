{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Content\n",
    "\n",
    "\n",
    " 1. Business Understanding (5 min)\n",
    "     * Objective\n",
    "     * Description\n",
    " 2. Data Understanding (15 min)\n",
    "    * Import Libraries\n",
    "    * Load data\n",
    "    * Statistical summaries and visualizations\n",
    " 3. Data Preparation (5 min)\n",
    "    * Missing values imputation\n",
    "    * Feature Engineering\n",
    " 4. Modeling (5 min)\n",
    "     * Select the model\n",
    "     * Build & Fit model\n",
    " 5. Evaluation (25 min)\n",
    "     * Model performance\n",
    "     * Hyperparameter tuning (We will not go over this for our demonstration)\n",
    " ![title](https://cdn-images-1.medium.com/max/1200/1*Ptsn7-aO_CTBQ1G5cOGN6Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "\n",
    "## 1.1 Objective\n",
    "Predict survival on the Titanic\n",
    "\n",
    "## 1.2 Description\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "Complete the analysis of what sorts of people were likely to survive. In particular, apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "\n",
    "## 2.1 Import Libraries\n",
    "First off some preparation. We need to import python libraries containing the necessary functionality we will need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Setup Functions\n",
    "There is no need to understand this code, just some hacks I had previously stashed away to help expedite ML projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load data\n",
    "Now that our packages are loaded, let's read in and take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0afa190419e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Dataset came in 2 csv files, labeled as train.csv and test.csv. Load both into Pandas Dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilepath_training_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\train.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfilepath_testing_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\test.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Dataset came in 2 csv files, labeled as train.csv and test.csv. Load both into Pandas Dataframes\n",
    "filepath_training_data = \"C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\train.csv\"\n",
    "train = pd.read_csv(filepath_training_data)\n",
    "\n",
    "filepath_testing_data = \"C:\\\\Users\\\\EISELJA\\\\Desktop\\\\DataScience\\\\Datasets\\\\test.csv\"\n",
    "test = pd.read_csv(filepath_testing_data)\n",
    "\n",
    "#Now that we have 2 dataframes, we need to append and combine into a full titanic dataframe\n",
    "full = train.append(test, ignore_index = True)\n",
    "\n",
    "#Rename to titanic and grab 891 rows and all columns\n",
    "titanic = full[:891]\n",
    "\n",
    "#Delete train and test dataframes, they are not needed anymore\n",
    "del train, test\n",
    "\n",
    "print(\"Datasets:\" , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Statistical summaries and visualisations\n",
    "\n",
    "To understand the data we are now going to consider some key facts about various features including their relationship with the target variable, i.e. survival.\n",
    "\n",
    "We start by looking at a few lines of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VARIABLE DESCRIPTIONS:**\n",
    "\n",
    "We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. To make things a bit more explicit since a couple of the variable names aren't 100% illuminating, here's what we've got to deal with:\n",
    "\n",
    "\n",
    "**Variable Description**\n",
    "\n",
    " - Survived: Survived (1) or died (0)\n",
    " - Pclass: Passenger's class\n",
    " - Name: Passenger's name\n",
    " - Sex: Passenger's sex\n",
    " - Age: Passenger's age\n",
    " - SibSp: Number of siblings/spouses aboard\n",
    " - Parch: Number of parents/children aboard\n",
    " - Ticket: Ticket number\n",
    " - Fare: Fare\n",
    " - Cabin: Cabin\n",
    " - Embarked: Port of where they left from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Next have a look at some key information about the variables\n",
    "A numeric variable is one with values of integers or real numbers while a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, such as blood type.\n",
    "\n",
    "Notice especially what type of variable each is, how many observations there are and some of the variable values.\n",
    "\n",
    "An interesting observation could for example be the minimum age 0.42, do you know why this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 A heat map of correlation may give us a understanding of which variables are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Further explore the relationship between the features and target variable (survival of passengers) \n",
    "We start by looking at the relationship between age and survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the graphs above. Differences between survival for different values is what will be used to separate the target variable (survival in this case) in the model. If the two lines had been about the same, then it would not have been a good variable for our predictive model. \n",
    "\n",
    "Consider some key questions such as; what age does males/females have a higher or lower probability of survival? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Embarked\n",
    "We can also look at categorical variables like Embarked and their relationship with survival.\n",
    "\n",
    "- C = Cherbourg  \n",
    "- Q = Queenstown\n",
    "- S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival rate by Pclass\n",
    "plot_categories(titanic, cat ='Pclass', target ='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival rate by SibSp\n",
    "plot_categories(titanic, cat ='SibSp', target ='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival rate by Parch\n",
    "plot_categories(titanic, cat ='Parch', target ='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Categorical variables need to be transformed to numeric variables\n",
    "The variables *Embarked*, *Pclass* and *Sex* are treated as categorical variables. Some of our model algorithms can only handle numeric values and so we need to create a new variable (dummy variable) for every unique value of the categorical variables.\n",
    "\n",
    "This variable will have a value 1 if the row has a particular value and a value 0 if not. *Sex*  will be encoded as one binary variable (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fill missing values in variables\n",
    "Most machine learning alghorims require all variables to have values in order to use it for training the model. The simplest method is to fill missing values with the average of the variable across all observations in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature Engineering &ndash; Creating new variables that did not explicitly exist in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Extract titles from passenger names\n",
    "Titles reflect social status and may predict survival probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Extract Cabin category information from the Cabin number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Extract ticket class from ticket number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Create family size and category for family size\n",
    "The two variables *Parch* and *SibSp* are used to create the famiy size variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Assemble final datasets for modelling\n",
    "\n",
    "Split dataset by rows into test and train in order to have a holdout set to do model evaluation on. The dataset is also split by columns in a matrix (X) containing the input data and a vector (y) containing the target (or labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Variable selection\n",
    "Select which features/variables to inculde in the dataset from the list below:\n",
    "\n",
    " - imputed \n",
    " - embarked\n",
    " - pclass\n",
    " - sex\n",
    " - family\n",
    " - cabin\n",
    " - ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Create datasets\n",
    "Below we will seperate the data into training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Feature importance\n",
    "Selecting the optimal features in the model is important. \n",
    "We will now try to evaluate what the most important variables are for the model to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "We will now select a model we would like to try then use the training dataset to train this model and thereby check the performance of the model using the test set. \n",
    "\n",
    "## 4.1 Model Selection\n",
    "Then there are several options to choose from when it comes to models. This is where Data Scientists make their money, knowing which model to apply when. But for the purposes of this Jupyter notebook we will go with a Logistic Regression model since our output or tagrte vaiable is survival represented as a binary value of 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train the selected model\n",
    "When you have selected a dataset with the features you want and a model you would like to try it is now time to train the model. After all our preparation model training is simply done with the one line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "Now we are going to evaluate model performance and the feature importance.\n",
    "\n",
    "## 5.1 Model performance\n",
    "We can evaluate the accuracy of the model by using the validation set (most of the time you will hear this called the test set) where we know the actual outcome. This data set have not been used for training the model, so it's completely new to the model. \n",
    "\n",
    "We then compare this accuracy score with the accuracy when using the model on the training data. If the difference between these are significant this is an indication of overfitting. We try to avoid this because it means the model will not generalize well to new data and is expected to perform poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_Y = model.predict( test_X )\n",
    "#passenger_id = full[891:].PassengerId\n",
    "#test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "#test.shape\n",
    "#test.head()\n",
    "#test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
